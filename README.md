# huggingface-llama-recipes

Local inference
* How to run Llama 8B in free Google Colab in half precision
* How to run Llama in 8-bit
* How to run Llama 3.1 405B quantized to INT4 with AWQ and GPTQ
* Doing some Python execution
* Custom tool usage

API inference
* How to use the Inference API for PRO users
* How to use a dedicated Inference Endpoint

Llama Guard
* Detecting jailbreaking with Prompt Guard
* Using Llama Guard for Guardrialing

Use cases
* Fine-tuning with TRL
* Synthetic Data Generation
* Assisted Decoding
* Data analyst agent
* Building a Gradio demo 
